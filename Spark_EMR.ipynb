{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Provision EMR Cluster\n",
    "```\n",
    "aws emr create-cluster --auto-scaling-role EMR_AutoScaling_DefaultRole \\\n",
    "--termination-protected --applications Name=Hadoop Name=Hive Name=Spark \\\n",
    "--bootstrap-actions '[{\"Path\":\"s3://chkrd/artifacts/emr-base.sh\",\"Args\":[\"--julia\",\"--ruby\",\"--ds-packages\",\"--ml-packages\",\"--python-packages\",\"ggplot nilearn spark-sklearn\"],\"Name\":\"EMR Base\"}]' \\\n",
    "--ec2-attributes '{\"KeyName\":\"devenv-key\",\"InstanceProfile\":\"EMR_EC2_DefaultRole\",\"SubnetId\":\"subnet-39a22e5e\",\"EmrManagedSlaveSecurityGroup\":\"sg-86bb82fe\",\"EmrManagedMasterSecurityGroup\":\"sg-83bb82fb\"}' \\\n",
    "--service-role EMR_DefaultRole --enable-debugging --release-label emr-5.1.0 \\\n",
    "--log-uri 's3n://chkrd2/elasticmapreduce/' --name 'SparkaaS' --instance-groups '[{\"InstanceCount\":2,\"InstanceGroupType\":\"CORE\",\"InstanceType\":\"m3.xlarge\",\"Name\":\"Core - 2\"},{\"InstanceCount\":1,\"InstanceGroupType\":\"MASTER\",\"InstanceType\":\"m3.xlarge\",\"Name\":\"Master - 1\"}]' \\\n",
    "--scale-down-behavior TERMINATE_AT_INSTANCE_HOUR --region us-west-2\n",
    "```\n",
    "\n",
    "# Load `sparkmagic` Jupyter Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Configure EMR Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added endpoint http://54.190.49.183:8998\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1489528374652_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-17-177.us-west-2.compute.internal:20888/proxy/application_1489528374652_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-25-192.us-west-2.compute.internal:8042/node/containerlogs/container_1489528374652_0004_01_000001/hadoop\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7fc919b38f50>"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of numbers is 1 and its description is:\n",
      "(2) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475 []"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "numbers = sc.parallelize([1, 2, 3, 4])\n",
    "print('First element of numbers is {} and its description is:\\n{}'.format(numbers.first(), numbers.toDebugString()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.SQLContext object at 0x7fc919846f10>"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fc919846c90>"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 4 times, most recent failure: Lost task 3.3 in stage 1.0 (TID 50, ip-172-31-16-152.us-west-2.compute.internal, executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/worker.py\", line 174, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/worker.py\", line 169, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/lib/python2.7/site-packages/spark_sklearn/grid_search.py\", line 232, in fun\n",
      "    return_parameters=True, error_score=error_score)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/cross_validation.py\", line 1665, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/ensemble/forest.py\", line 247, in fit\n",
      "    X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/utils/validation.py\", line 382, in check_array\n",
      "    array = np.array(array, dtype=dtype, order=order, copy=copy)\n",
      "ValueError: could not convert string to float: Q\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/worker.py\", line 174, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/worker.py\", line 169, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/lib/python2.7/site-packages/spark_sklearn/grid_search.py\", line 232, in fun\n",
      "    return_parameters=True, error_score=error_score)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/cross_validation.py\", line 1665, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/ensemble/forest.py\", line 247, in fit\n",
      "    X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/utils/validation.py\", line 382, in check_array\n",
      "    array = np.array(array, dtype=dtype, order=order, copy=copy)\n",
      "ValueError: could not convert string to float: Q\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/spark_sklearn/grid_search.py\", line 181, in fit\n",
      "    return self._fit(X, y, ParameterGrid(self.param_grid))\n",
      "  File \"/usr/local/lib/python2.7/site-packages/spark_sklearn/grid_search.py\", line 234, in _fit\n",
      "    indexed_out0 = dict(par_param_grid.map(fun).collect())\n",
      "  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000001/pyspark.zip/pyspark/rdd.py\", line 809, in collect\n",
      "    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000001/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000001/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 4 times, most recent failure: Lost task 3.3 in stage 1.0 (TID 50, ip-172-31-16-152.us-west-2.compute.internal, executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/worker.py\", line 174, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/worker.py\", line 169, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/lib/python2.7/site-packages/spark_sklearn/grid_search.py\", line 232, in fun\n",
      "    return_parameters=True, error_score=error_score)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/cross_validation.py\", line 1665, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/ensemble/forest.py\", line 247, in fit\n",
      "    X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/utils/validation.py\", line 382, in check_array\n",
      "    array = np.array(array, dtype=dtype, order=order, copy=copy)\n",
      "ValueError: could not convert string to float: Q\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/worker.py\", line 174, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/worker.py\", line 169, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1489528374652_0003/container_1489528374652_0003_01_000008/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/lib/python2.7/site-packages/spark_sklearn/grid_search.py\", line 232, in fun\n",
      "    return_parameters=True, error_score=error_score)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/cross_validation.py\", line 1665, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/ensemble/forest.py\", line 247, in fit\n",
      "    X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n",
      "  File \"/usr/local/lib64/python2.7/site-packages/sklearn/utils/validation.py\", line 382, in check_array\n",
      "    array = np.array(array, dtype=dtype, order=order, copy=copy)\n",
      "ValueError: could not convert string to float: Q\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from spark_sklearn import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# Databrix utility function to report top 3 best scores\n",
    "def report(grid_scores, n_top = 3):\n",
    "    top_scores = sorted(grid_scores, key = itemgetter(1), reverse = True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "# Load the train and test datasets to create two DataFrames\n",
    "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "train = pd.read_csv(train_url)\n",
    "test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
    "test = pd.read_csv(test_url)\n",
    "\n",
    "# Creat a list of the features. \n",
    "target = train[\"Survived\"].values\n",
    "forest_features = train[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Add grid parameter code based on Databricks\n",
    "param_grid = {\"max_depth\": [3, 10, None],\n",
    "              \"min_samples_split\": [1.0, 2, 3, 10], \n",
    "              \"min_samples_leaf\": [1.0, 2, 3, 10], \n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"n_estimators\": [10, 20, 40, 80, 160]}\n",
    "\n",
    "# Execute the grid search\n",
    "Spark_gs = GridSearchCV(sc, RandomForestClassifier(), param_grid = param_grid)\n",
    "start = time()\n",
    "\n",
    "Spark_gs.fit(forest_features, target)\n",
    "print(\"Spark GridSearchCV took {:.2f} seconds for {:d} candidate settings.\".format(time() - start, len(Spark_gs.grid_scores_)))\n",
    "report(Spark_gs.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "from sklearn import grid_search, datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Use spark_sklearnâ€™s grid search instead:\n",
    "from spark_sklearn import GridSearchCV\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1.0, 3, 10],\n",
    "              \"min_samples_split\": [1.0, 3, 10],\n",
    "              \"min_samples_leaf\": [3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"n_estimators\": [10, 20, 40, 80]}\n",
    "gs = grid_search.GridSearchCV(RandomForestClassifier(), param_grid=param_grid)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# T?E?ST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 488.55 seconds for 864 %%sdpark settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.949 (std: 0.005)\n",
      "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 80, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': 3, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.946 (std: 0.006)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 80, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 1, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.944 (std: 0.004)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 80, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 3, 'max_depth': None}"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# Imports and utilities\n",
    "import numpy as np\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from sklearn import svm, grid_search, datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from spark_sklearn import GridSearchCV\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [1.0, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"n_estimators\": [10, 20, 40, 80]}\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "gs = grid_search.GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "gs.fit(X, y)\n",
    "print(\"GridSearchCV took {:.2f} seconds for {:d} %%sdpark settings.\".format(time() - start, len(gs.grid_scores_)))\n",
    "report(gs.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 64.37 seconds for 864 candidate settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.949 (std: 0.009)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 80, 'min_samples_split': 10, 'criterion': 'gini', 'max_features': 3, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.945 (std: 0.004)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 80, 'min_samples_split': 10, 'criterion': 'entropy', 'max_features': 3, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.945 (std: 0.008)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 40, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 3, 'max_depth': None}"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "from spark_sklearn import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(sc, clf, param_grid)\n",
    "start = time()\n",
    "gs.fit(X, y)\n",
    "print(\"GridSearchCV took {:.2f} seconds for {:d} candidate settings.\".format(time() - start, len(gs.grid_scores_)))\n",
    "report(gs.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "00659241dd2043b08943686087b1d01e": {
     "views": []
    },
    "00952c350e744dcfa1b2f6593db85f42": {
     "views": []
    },
    "01cc1fcd394c4c94b256ed39653448ea": {
     "views": []
    },
    "01ef7b8a57e54296914a84066da5ed6d": {
     "views": []
    },
    "02c78868f4d34baea50519a6efdd0837": {
     "views": []
    },
    "02ddfad741cc4985916beea443a2e06e": {
     "views": []
    },
    "054e15978419492ab9386e0e4ba508ed": {
     "views": []
    },
    "06061a4df4534f68884c082dcd4d31fa": {
     "views": []
    },
    "063390e4e4fd44f4b88720a728aa085c": {
     "views": []
    },
    "06c1033fc436416cac364d150f0e44dd": {
     "views": []
    },
    "06d782cb04b542acb8f66f10af48dbd9": {
     "views": []
    },
    "085cfd84261f4b2ca0d883c64cd9fc23": {
     "views": []
    },
    "08ac67f0c07e497bac5e107c73a65bb2": {
     "views": []
    },
    "0a1de8311f8a42929f86d123bb6ea239": {
     "views": []
    },
    "0a67290c565f41499625a40080b8ba53": {
     "views": []
    },
    "0ab18bf021ce4ed9aceb51cad5c04293": {
     "views": []
    },
    "0b7747db49294935a65b9f72e0e58d3a": {
     "views": []
    },
    "0ffe93499176426f8341afb8187d31ff": {
     "views": []
    },
    "11118c0fb456450f959bf158c7f364c4": {
     "views": []
    },
    "11cd747ac6ec47fbb736f63a90ae9c66": {
     "views": []
    },
    "1382202413014a069a7855f71abb09a9": {
     "views": []
    },
    "146efb56549445b784cb4c4160a75640": {
     "views": []
    },
    "14fb3a901f8d4173991f75fde5dee57c": {
     "views": []
    },
    "1569ccf2b1cd4883a02276cc42cdf174": {
     "views": []
    },
    "158802ce86af400fbff0eb2d98f5db01": {
     "views": []
    },
    "1674e2c474cc40c7a164b653302ce8c6": {
     "views": []
    },
    "183d66a3eb6b4afbb27233c9e7e7c983": {
     "views": []
    },
    "189ad44e1ddd41dd9eceac5dbed81335": {
     "views": []
    },
    "18acf7805d4e48b1830f8eac39883b16": {
     "views": []
    },
    "1ae12687f1bd4354b8c2d694bdefc877": {
     "views": []
    },
    "1d9f592dcff4438987fa4b7f518eb8e5": {
     "views": []
    },
    "1e8089f9e65d430daa3bc581b924df88": {
     "views": []
    },
    "1ea1fd9b8d894af98a6e7f6cf60bf820": {
     "views": []
    },
    "1effd4e2670d41f1b8dbb3b51d581a01": {
     "views": []
    },
    "1f8b8956174d483f92129951227cd612": {
     "views": []
    },
    "2149dbf3e0634413872a640a406e8a98": {
     "views": []
    },
    "23aac4aea10f4f0783eb3f3aeadb6135": {
     "views": []
    },
    "23bd861bf8f745bf80b3cd66fce9ade3": {
     "views": []
    },
    "253a204067944ed3811af678a67e2a41": {
     "views": []
    },
    "26d1c12a5813405a9edebaef9cbb7b79": {
     "views": []
    },
    "27921da40ad0416b81b9d4af0635db97": {
     "views": []
    },
    "28e547288b7045518a0846955f6e3528": {
     "views": []
    },
    "29296d1d6a164a59bd1df6941db990a3": {
     "views": []
    },
    "2c50412ef9934592972b240830dd4c6c": {
     "views": []
    },
    "2d1ca03632a641a4914fa566c8e2c394": {
     "views": []
    },
    "300f25ca12524112ac96c5c735c91cf1": {
     "views": []
    },
    "32919765a5e34dce9e2f03b530112ac9": {
     "views": []
    },
    "33c3c326be8e425eb6021bbcbb572d3b": {
     "views": []
    },
    "36755fbde1b241b88fffd8df321ca1de": {
     "views": []
    },
    "37134e3240c045ac823abebf3ea4d281": {
     "views": []
    },
    "38d9eea12bd34e8093e98664d29a1834": {
     "views": []
    },
    "3a627fb3ff6f44f98cc5a5712f65eb34": {
     "views": []
    },
    "3a9c9f8f5aa44263896b0445b6446d13": {
     "views": []
    },
    "3dd997ab2ea44464af87b75e9b820d71": {
     "views": []
    },
    "40ac8fa8d35046e1972d6641b8b79bb0": {
     "views": []
    },
    "422060993927441bb15c919f5193e4a9": {
     "views": []
    },
    "43d6c483422648e096836f305738911d": {
     "views": []
    },
    "4478094b9e614e0da64eeee37b966d42": {
     "views": []
    },
    "45504058590a4610ae26815b3593d022": {
     "views": []
    },
    "459008d7f3cc446090145807d144d7aa": {
     "views": []
    },
    "4875c571be944fcabdf4ca8c2bf0df77": {
     "views": []
    },
    "4ae5f1f587f34a52a64c510c9e6bb2d0": {
     "views": []
    },
    "4b2303bff9724d37ab93bcb3e48d86d0": {
     "views": []
    },
    "4b4a7f5133d448c7ab74d5cd5b4ce4bc": {
     "views": []
    },
    "4c21d8c294204bbda39447f66f7027ae": {
     "views": []
    },
    "4dd97c1219b348bbb033f68fdef2c46c": {
     "views": []
    },
    "4df007efa7e84b0dbdcdf1952352cee4": {
     "views": []
    },
    "50aaed4b8c604e5693af336d7bf6a197": {
     "views": []
    },
    "50d4b408a96c43179a79254c4299ef31": {
     "views": []
    },
    "534966c434f74170a2d3950242268a76": {
     "views": []
    },
    "54f7da6d3614445d813a7226d6bc4b0e": {
     "views": []
    },
    "56b290095f4b4dc58f2750895a739197": {
     "views": []
    },
    "58293c55d5b04a9ab55fbb1d90bcc669": {
     "views": []
    },
    "5844a1c1bef24f6c83b183c242c73f1a": {
     "views": []
    },
    "5a9874455e6a4abcb77c94cff2c2d0fa": {
     "views": []
    },
    "5b15bb569acb4cf5b65c25873d3983b6": {
     "views": []
    },
    "5b6c86be3bd945398eb417ba2cba2773": {
     "views": []
    },
    "5cce92ba0540465b86ac89f3d7a162b2": {
     "views": []
    },
    "5dbcd97a71d84e4cac8dc7128f0850d6": {
     "views": []
    },
    "5e97c08aaddb49c3afaeb81a4896b93f": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    },
    "5f051d13c1b04d6f83b7186291a6165f": {
     "views": []
    },
    "5f5fe274dae24e0cb8dfef128eae46c5": {
     "views": []
    },
    "6072d0123f3e40c9a83976bec4d62e26": {
     "views": []
    },
    "6486a78c02c2499f83e49e7ce92e3dd2": {
     "views": []
    },
    "64f4d050f0bf4e169b6a42ff1f88dfe5": {
     "views": []
    },
    "66f3e248c09f47bfa18930ff5251b80a": {
     "views": []
    },
    "68a1e6d660774c23a95a4188133ef2d2": {
     "views": []
    },
    "68fa55e7c53d4b1ab7e65eb0a16eb85c": {
     "views": []
    },
    "6932a106a3714de189c556b773d66823": {
     "views": []
    },
    "6a01dfd6bb8c4986be86f22a478b22cd": {
     "views": []
    },
    "6c3004caae134ddbb4313a8aeb569274": {
     "views": []
    },
    "6f9feb4c2deb4c5da63304581408b8f3": {
     "views": []
    },
    "704f49406e374f4483dd495e17fd28a0": {
     "views": []
    },
    "711862ed4ad847d4958f759a176ccb64": {
     "views": []
    },
    "714b44b1ee1f4471a1536229406d7e7d": {
     "views": []
    },
    "724cf060fee941cf8883a546ae32ba3f": {
     "views": []
    },
    "74ee264b0b3142fabb4745b0acf09c10": {
     "views": []
    },
    "752fbdbbe49f4a2ca554ffe6666acd9b": {
     "views": []
    },
    "75e89789da6e4892a793876c6c8fe1dd": {
     "views": []
    },
    "760ba9b826de4df989db30ec06102cd5": {
     "views": []
    },
    "76686f58d45d4b40a618ccc76f0a632f": {
     "views": []
    },
    "76e27062001947418ae1ac1f6fe4bd0e": {
     "views": []
    },
    "77a2c16bbd2041498b2a82175da461aa": {
     "views": []
    },
    "7801865934e9484490ad6dbf2a32a0d0": {
     "views": []
    },
    "7abae47303294efca617565d7345bd36": {
     "views": []
    },
    "7b95c432c52c40c7a081ba8d0e0ed90c": {
     "views": []
    },
    "7f9f0e0ee8254dd480540d8ddf0b9503": {
     "views": []
    },
    "7ff6d5bd3bdc4e98b6750d6ce66f7a46": {
     "views": []
    },
    "808195b1879342df81ace825ab5cdd06": {
     "views": []
    },
    "8089796907654716baf4d4ae21144b62": {
     "views": []
    },
    "8170059c01174720a6ff6c9e39341f10": {
     "views": []
    },
    "8254b30a8a2c4439a39f406b1b22d270": {
     "views": []
    },
    "84601ff1a9234ac58b10e46d42782a78": {
     "views": []
    },
    "88ddfe4b96b84bef93519bcdf056a85f": {
     "views": []
    },
    "896e94189564403985500bfd1122c2ff": {
     "views": []
    },
    "8a02f8d850c34971b5a4f1cd8cb81c23": {
     "views": []
    },
    "8a0ab42180f64a87858589b9289a990a": {
     "views": []
    },
    "8a9436fea5e347468545121e8566f9e9": {
     "views": []
    },
    "8a9bc7b8de774a3989e52a65abf3803a": {
     "views": []
    },
    "8c6175841d75477981518ae2bc152ff1": {
     "views": []
    },
    "8d4f26847bb647efa7221fa73d02c4a1": {
     "views": []
    },
    "8e3e437364264a2d82e99fc7953d1c1d": {
     "views": []
    },
    "8f1dca2ecce348a29bb28d009bc0da26": {
     "views": []
    },
    "92e5d8160c544d05bbae6a60dada60d1": {
     "views": []
    },
    "945d639b3b7f45f087317e4608bdd2e3": {
     "views": []
    },
    "95521fb7ef5f49d1985f54543592735d": {
     "views": []
    },
    "9592291fb0a94ebfbe4b16f6827f2b58": {
     "views": []
    },
    "95b39e2ea1384532a19e203ea635fb32": {
     "views": []
    },
    "95d3ae02ec2946f19e8aac64048e778e": {
     "views": []
    },
    "95da3b0192f24664a557db7f1752facc": {
     "views": []
    },
    "9c0f8cbe5d1a4027801e04b7fbe0d154": {
     "views": []
    },
    "9c478c0eefd84bfa932452915d5d8656": {
     "views": []
    },
    "9ccecad9d7964da28563584cc6b264a8": {
     "views": []
    },
    "9e19bc31915c42318a675012daaa7eff": {
     "views": []
    },
    "9f046e630ec4417eb9ab54cb78cd315f": {
     "views": []
    },
    "9fdf2491edb3413ab21ab3806714675d": {
     "views": []
    },
    "a165f202569a4136887ec4c3d591b7d3": {
     "views": []
    },
    "a2997f40979f47d882d8775d5f5e6082": {
     "views": []
    },
    "a46a50c6cd1d483e914c7000a553f788": {
     "views": []
    },
    "a6c4474f7f674c35acf39b13959add9d": {
     "views": []
    },
    "a76c8abb032a4a418f4b86a9fe391c54": {
     "views": []
    },
    "a7feff3e14f8465c80f225e143f82d80": {
     "views": []
    },
    "ac1d0f6e8ff94de0a377c7cb22f4b8e6": {
     "views": []
    },
    "ac42a25733f54cccb24371f30a391db7": {
     "views": []
    },
    "ae54e57b51914e5189efc0799fbcbc3a": {
     "views": []
    },
    "b1a3459be8134ec79eac913279831ee3": {
     "views": []
    },
    "b3eabe45b4514b209a3bc9af744ac238": {
     "views": []
    },
    "b5932a9c6aac4c7bb0a01a91a6befc1e": {
     "views": []
    },
    "b807a66acc79494393cfe6affacbb9e1": {
     "views": []
    },
    "b89887a1edd54748abe870dfd377a3e6": {
     "views": []
    },
    "b9a1c40ebd1e4cbe90a82799e4c4f10d": {
     "views": []
    },
    "b9d464bcc60d49be98b93e78f395dd3e": {
     "views": []
    },
    "bd15c285e28144638ca6d87d0fa150eb": {
     "views": []
    },
    "bdd841920c1642a0a1a0a1005cae17e4": {
     "views": []
    },
    "bf7ccf0b1f5847c1aa9e4bb179f035fb": {
     "views": []
    },
    "bfe6466890834c91b343e1a687054125": {
     "views": []
    },
    "c19b10f8f0414027a63dd7cdff7145f6": {
     "views": []
    },
    "c282626eb7b6477bb50575587dcd46d7": {
     "views": []
    },
    "c2bb9fd3f91444b19401de55c6dc82e6": {
     "views": []
    },
    "c3563514ee1347f7b585f4f31583e336": {
     "views": []
    },
    "c50a969ac15841cea6f4626ceaaf51af": {
     "views": []
    },
    "c6d57dfd4aca4d098773b10e2c381e3b": {
     "views": []
    },
    "c8ed5f5371e8414cbc3aa2f2350263aa": {
     "views": []
    },
    "cf10261f3a13496886cf70bb5147c042": {
     "views": []
    },
    "d0020202a5b84cf9b2fae671e8fdb52f": {
     "views": []
    },
    "d0da9f06c0154fe7b76d463cf9b70634": {
     "views": []
    },
    "d2d7b4f443284d268c7fe5c40300b2a6": {
     "views": []
    },
    "d35c04136e9d47cb990a95eab03dc4d3": {
     "views": []
    },
    "d5aa0248e2a8485f8e2fdb9173f1167a": {
     "views": []
    },
    "d98156a9eec54f3aba1182e8e7b3afc7": {
     "views": []
    },
    "da824e682c5647f3a783c443b066d6d2": {
     "views": []
    },
    "da8c0c765f6345718bb76bcd906c0fc0": {
     "views": []
    },
    "dd905bd9c8214b77b48b4b6f2d7227fb": {
     "views": []
    },
    "e028c845a90f4f7092b00663aaece5b7": {
     "views": []
    },
    "e0df6954487b440bb25cfb469d48ebc3": {
     "views": []
    },
    "e1dabac284c54036a134854f85025491": {
     "views": []
    },
    "e24337f6017341098e3bb7471190b156": {
     "views": []
    },
    "e2622f2191294329ace11d42492e73b3": {
     "views": []
    },
    "e3b97bcf4bd64c158dd8eadfe1c04b53": {
     "views": []
    },
    "e442f4a2637b4308ae4794e41cfab051": {
     "views": []
    },
    "e5a6352fa19f4bd1917077e1976cb6f6": {
     "views": []
    },
    "e5f781f4a4b241b68d717799fd330e94": {
     "views": []
    },
    "e7514bc0169c4a72a7349669f29833e4": {
     "views": []
    },
    "e7640c3627254a4799350569afc703b2": {
     "views": []
    },
    "e8d24584caaa4ed09f4d77378af8ee63": {
     "views": []
    },
    "e8f2679866ae4ca994fb35377664f530": {
     "views": []
    },
    "e94bd0d4debb4900a045a07d14067bc5": {
     "views": []
    },
    "ea2a2d010c4b4e0d805ede1c300d1e39": {
     "views": []
    },
    "eb83d763d2444ee4ba0dc76259d7a2b2": {
     "views": []
    },
    "f01db6dee55d4170b6712454e51d787a": {
     "views": []
    },
    "f2a7b84a94634095a3a28f41fea013ad": {
     "views": []
    },
    "f2cba9b381ab41f49b38b705444e7dc5": {
     "views": []
    },
    "f42a38c359634341b36868c716146569": {
     "views": []
    },
    "f501165ef3d24b2aa6602ca71534db06": {
     "views": []
    },
    "f65e3635b0ae4f49a9cc76708d90d821": {
     "views": []
    },
    "f9a4d05cc1ad4c1db20832c398efbb09": {
     "views": []
    },
    "f9a69dd03c7441eea7ebd8a3d23e7257": {
     "views": []
    },
    "fa1e9d73bf3348dd9cb109f030beeb06": {
     "views": []
    },
    "fb00c66700be410eaa3201778bdcc82d": {
     "views": []
    },
    "fb87558b7d4442f792a15a11086b7599": {
     "views": []
    },
    "fba11b0b49444ab79db3833cfded9e23": {
     "views": []
    },
    "fc45107bf1774f1b99d08e9448a3fb4b": {
     "views": []
    },
    "fd6253c1b522442b85509f47973a9714": {
     "views": []
    },
    "ffc47569e7874e1989f9b36748fca1f5": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
