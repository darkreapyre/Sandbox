{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction\n",
    "In this case study, we will explore how to tackle Kaggle Titanic competition using Python and Machine Learning. When the Titanic sank, $1502$ of the $2224$ passengers and crew were killed. One of the main reasons for this high level of casualties was the lack of lifeboats on this self-proclaimed __\"unsinkable\"__ ship. In this tutorial, we will learn how to apply machine learning techniques to predict a passenger's chance of surviving using Python.\n",
    "\n",
    "# Getting Data with Pandas\n",
    "We start with loading in the training and testing set into your Python environment. We will use the [training set](http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv) to build our model, and the [test set](http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv) to validate it. The first step is to load this data with the `read_csv()` method from the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "# Import the Pandas library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# Load the train and test datasets to create two DataFrames\n",
    "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "train = pd.read_csv(train_url)\n",
    "test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
    "test = pd.read_csv(test_url)\n",
    "\n",
    "#Print the `head` of the train dataframe\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "#Print the `head` of the test dataframe\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On thing that immediately stands out when looking at the two data sets. The `test` set has no variable (colum) for wether or not the passanger `Survived` or not. This has been intentionally removed as that's the variable we will be predicting using the `train` set.\n",
    "\n",
    "# Exploring the Data\n",
    "Before starting with the actual analysis, it's important to understand the structure of the data. Both `test` and `train` are DataFrame objects, the way pandas represent datasets. We can easily explore a DataFrame using the `.describe()` method. This method summarizes the columns/features of the DataFrame, including the count of observations, mean, max and so on. Another useful trick is to look at the dimensions of the DataFrame. This is done by requesting the `.shape` attribute of your DataFrame object. It is also a good practice to look for any missing values in the data set.\n",
    "\n",
    "### Summary Statistics\n",
    "Next we apply the `.describe()` method, look for missing values and then apply `.shape` attribute of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "# Describe the `train` data\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "# Look for missing values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61     NaN\n",
       "829    NaN\n",
       "Name: Embarked, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "# Index for missing vales: `Embarked`\n",
    "train[\"Embarked\"][train[\"Embarked\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "# Look at the dimensions of `train`\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Understanding the Data\n",
    "As we can see, the training set has $891$ observations and $12$ variables, the count for `Age` is $714$. But how many people in the training set survived the disaster with the Titanic? To see this, we can use the `value_counts()` method in combination with standard bracket notation to select a single column of a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    342\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "# No. of people who survived (absolute numbers)\n",
    "train[\"Survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    61.616162\n",
       "1    38.383838\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "# No. of people who survived (percentages)\n",
    "train[\"Survived\"].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We see that $549$ individuals died ($62\\%$) and $342$ survived ($38\\%$). A simple way to predict heuristically could be: \"majority wins\". This would mean that we will predict every unseen observation to not survive.\n",
    "\n",
    "To dive in a little deeper we can perform similar counts and percentage calculations on subsets of the Survived column. For example, maybe gender could play a role as well? We can explore this using the .`value_counts()` method for a two-way comparison on the number of __males__ and __females__ that survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    468\n",
       "1    109\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "# Count of males who survived\n",
    "train[\"Survived\"][train[\"Sex\"] == \"male\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    233\n",
       "0     81\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "# Count of femails who survived\n",
    "train[\"Survived\"][train[\"Sex\"] == \"female\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To get proportions,  we again pass in the argument `normalize = True` to the `.value_counts()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    81.109185\n",
       "1    18.890815\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "# Count of males who survived (percentage)\n",
    "train[\"Survived\"][train[\"Sex\"] == \"male\"].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    74.203822\n",
       "0    25.796178\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "# Count of females who survived (percentage)\n",
    "train[\"Survived\"][train[\"Sex\"] == \"female\"].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It looks like it makes sense to include gender in the predictions since there is a difference between the survival rate of males vs. females. Around $74\\%$ of females survived as opposed to $18\\%$ of the males surviving.\n",
    "\n",
    "Another variable that could influence survival is `age`; since it's probable that children were saved first. We can test this by creating a new column with a categorical variable `Child`. `Child` will take the value $1$ in cases where age is less than $18$, and a value of $0$ in cases where age is greater than or equal to $18$. So to add this new variable we need to do two things:\n",
    "\n",
    "1. Create a new column.\n",
    "2. Provide the values for each observation (i.e., row) based on the age of the passenger.\n",
    "\n",
    "Adding a new column with Pandas in Python is easy and can be done via the following syntax:\n",
    "```\n",
    "<variable>[\"new_variable\"] = 0\n",
    "```\n",
    "This code would create a new column in the train DataFrame titled new_var with $0$ for each observation. To set the values based on the age of the passenger, we make use of a boolean test inside the square bracket operator. With the `[]` operator we create a subset of rows and assign a value to a certain variable of that subset of observations. For example:\n",
    "\n",
    "```\n",
    "train[\"new_var\"][train[\"Fare\"] > 10] = 1\n",
    "```\n",
    "\n",
    "This would give a value of $1$ to the variable `new_var` for the subset of passengers whose fares greater than $10$. Keeping in mind that `new_var` has a value of $0$ for all other values (including missing values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival proportions for passangers under 18:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    53.982301\n",
       "0    46.017699\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "# Create the column Child and assign to 'NaN'\n",
    "train[\"Child\"] = float('NaN')\n",
    "\n",
    "# Assign 1 to passengers under 18, 0 to those 18 or older.\n",
    "train[\"Child\"][train[\"Age\"] < 18] = 1\n",
    "train[\"Child\"][train[\"Age\"] >= 18] = 0\n",
    "\n",
    "# Print normalized Survival Rates for passengers under 18\n",
    "print \"Survival proportions for passangers under 18:\\n\",\n",
    "train[\"Survived\"][train[\"Child\"] == 1].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival proportions for passangers over 18:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    61.896839\n",
       "1    38.103161\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "# Print normalized Survival Rates for passengers 18 or older\n",
    "print \"Survival proportions for passangers over 18:\\n\",\n",
    "train[\"Survived\"][train[\"Child\"] == 0].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we can see from the survival proportions, age does certainly seem to play a role. So the the __[Birhenhead Drill](https://en.wikipedia.org/wiki/Women_and_children_first)__ holds true and thus `Sex` and `Age` make good predictors.\n",
    "\n",
    "# Basic Prediction\n",
    "From exploring the data we can see that females had over a $50\\%$ chance of surviving and males had less than a $50\\%$ chance of surviving. Hence, we could use this information for a first and very basic prediction: \n",
    "\n",
    "__All females in the `test` set survive and all males in the `test` set die.__\n",
    "\n",
    "To do this, we use the test set for validating our predictions. As was mentioned above,  the `test` set has no `Survived` column. this is so that we can use this colums for our predicted values. Next, when uploading our results, Kaggle will use this variable i.e. oour predictions, to score the performance. \n",
    "\n",
    "So to start with the first prediction, we will perform the following:\n",
    "\n",
    "1. Create a variable test_one, identical to dataset test.\n",
    "2. Add an additional column, `Survived`, that is initialize to zero.\n",
    "3. Use vector subsetting to set the value of `Survived` to $1$ for observations whose Sex equals \"female\".\n",
    "4. Print the Survived column of predictions from the test_one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "# Start the timer\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "# Create a copy of test: test_one\n",
    "test_one = test\n",
    "\n",
    "# Initialize a Survived column to 0\n",
    "test_one[\"Survived\"] = 0\n",
    "\n",
    "# Set Survived to 1 if Sex equals \"female\"\n",
    "test_one[\"Survived\"][test_one[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "# Print a sample prediction of who servived\n",
    "test_one[[\"PassengerId\", \"Survived\"]] .head()\n",
    "#print \"Our basic prediction took {:.2f} seconds.\".format(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Prediction using Decision Trees\n",
    "In the basic prediction example, we did all the \"slicing\" and \"dicing\" ourselves to find subsets that have a higher chance of surviving. A decision tree automates this process for us and outputs a classification model or classifier.\n",
    "\n",
    "Conceptually, the decision tree algorithm starts with all the data at the root node and scans all the variables for the best one to split on. Once a variable is chosen, it does the split and goes down one level (or one node) and repeats the process. The final nodes at the bottom of the decision tree are known as terminal nodes, and the majority vote of the observations in that node determine how to predict for new observations that end up in that terminal node.\n",
    "\n",
    "Before we can start using Decision Trees, we need to import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "# Import the Numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Import 'tree' from scikit-learn library\n",
    "from sklearn import tree\n",
    "\n",
    "# Reload the train and test datasets to create two DataFrames\n",
    "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "train = pd.read_csv(train_url)\n",
    "test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
    "test = pd.read_csv(test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing\n",
    "Before we can begin constructing your trees we need to clean the data so that we can use all the features (predictors) available. In the first section, we saw that the `Age` variable had some missing value. Although dealing with missing values is a whole subject with and in itself, we will use a simple imputation technique where we substitute each missing value with the median of the all present values. This is done by using the `.fillna()` method, for example:\n",
    "```\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n",
    "```\n",
    "Another problem is that the `Sex` and `Embarked` variables are categorical but in a non-numeric format. Thus, we will need to assign each class a unique integer so that Python can handle the information. `Embarked` also has some missing values which we should impute with the most common class of embarkation, which is \"S\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of missing values:  PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Convert the male and female groups to integer form\n",
    "train[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\n",
    "train[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "# Impute the `Embarked` variable\n",
    "train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# Impute the `Age` variable\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n",
    "\n",
    "# Confirm that `Embarked` and `Age` have no missing values\n",
    "print \"No. of missing values: \", train.isnull().sum()\n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"S\"] = 0\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"C\"] = 1\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"Q\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fitting the Model\n",
    "Now that the data has been cleaned, we will use the scikit-learn and numpy libraries to build a decision tree. scikit-learn can be used to create tree objects from the `DecisionTreeClassifier` class. The methods that we will use take numpy arrays as inputs and therefore we will need to create those from the DataFrame that we already have. \n",
    "\n",
    "We will need the following to build a decision tree\n",
    "\n",
    "- `target`: A one-dimensional numpy array containing the target/response from the train data. (`Survival`)\n",
    "- `features`: A multidimensional numpy array containing the features/predictors from the train data. (e.g. `Sex`, `Age`)\n",
    "\n",
    "The following sample code shows what this would look like:\n",
    "```\n",
    "target = train[\"Survived\"].values\n",
    "\n",
    "features = train[[\"Sex\", \"Age\"]].values\n",
    "\n",
    "my_tree = tree.DecisionTreeClassifier()\n",
    "\n",
    "my_tree = my_tree.fit(features, target)\n",
    "```\n",
    "One way to quickly see the result of the decision tree is to see the importance of the features that are included. This is done by requesting the `.feature_importances_` attribute of the tree object. Another quick metric is the mean accuracy that we can compute using the `.score()` function with `features_one` and `target` as arguments.\n",
    "\n",
    "To build the decision tree, we will perform the following steps:\n",
    "1. Build the `target` and `features_one` numpy arrays. The target will be based on the `Survived` column in `train`. The features array will be based on the variables `Passenger`, `Class`, `Sex`, `Age`, and Passenger `Fare`.\n",
    "2. Build a decision tree `my_tree_one` to predict survival using `features_one` and `target`.\n",
    "3. View at the importance of features in the decision tree and compute the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Decision Tree prediction took 0.00 seconds.\n",
      "Importance:\n",
      "[ 0.12901815  0.31274009  0.22207249  0.33616926]\n",
      "Score:\n",
      "0.977553310887\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Create the target and features numpy arrays: target, features_one\n",
    "target = train[\"Survived\"].values\n",
    "features_one = train[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "# Fit your first decision tree: my_tree_one\n",
    "my_tree_one = tree.DecisionTreeClassifier()\n",
    "start = time()\n",
    "my_tree_one = my_tree_one.fit(features_one, target)\n",
    "\n",
    "# Look at the importance and score of the included features\n",
    "print \"Our Decision Tree prediction took {:.2f} seconds.\".format(time() - start)\n",
    "print \"Importance:\\n\", my_tree_one.feature_importances_\n",
    "print \"Score:\\n\", my_tree_one.score(features_one, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looks like assenger __Fare__ has most significance in determining survival based on the model. Since we decalred the features to use (`features_one`), we can assume that the important features are assigned in the same order, but let's confirm that by mapping the feature name to it's importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "[(0.3362, 'Fare'), (0.3127, 'Sex'), (0.2221, 'Age'), (0.129, 'Pclass')]\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# List of feature names\n",
    "names = [\"Pclass\", \"Sex\", \"Age\", \"Fare\"]\n",
    "\n",
    "# Code coutesy of:\n",
    "# http://blog.datadive.net/selecting-good-features-part-iii-random-forests/\n",
    "print \"Features sorted by their score:\"\n",
    "print sorted(zip(map(lambda x: round(x, 4), my_tree_one.feature_importances_), names), \n",
    "             reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dealing with Overfitting\n",
    "When applying models to new data, one thing to pay special attention to is __[overfitting](https://en.wikipedia.org/wiki/Overfitting)__. when creating the decision tree above, the default arguments for `max_depth` and `min_samples_split` were set to `None`. This means that no limit on the depth of the tree was set. This is not necessary a good thing, as we are likely overfitting. This means that while our model describes the training data extremely well, it doesn't generalize to new data, which is frankly the point of prediction. \n",
    "\n",
    "One solution to address this is to make  a less complex model. In `DecisionTreeRegressor`, the depth of the model is defined by two parameters:\n",
    "- The `max_depth` parameter determines when the splitting up of the decision tree stops.\n",
    "- The `min_samples_split` parameter monitors the amount of observations in a bucket. If a certain threshold is not reached (e.g minimum 10 passengers) no further splitting can be done.\n",
    "\n",
    "By limiting the complexity of the decision tree we can increase its generality and thus its usefulness for better prediction. To test this theory we now include the Siblings or Spouses Aboard (`SibSp`), Parents/Children Aboard (`Parch`), and `Embarked` features in a new set of features and fit a  second tree (`my_tree_two`) with the new features, and control for the model compelexity by toggling the `max_depth` and `min_samples_split` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our new Decision Tree prediction took 0.00 seconds.\n",
      "Importance:\n",
      "[ 0.14130255  0.17906027  0.41616727  0.17938711  0.05039699  0.01923751\n",
      "  0.0144483 ]\n",
      "Score:\n",
      "0.905723905724\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Create a new array with the added features: features_two\n",
    "target = train[\"Survived\"].values\n",
    "features_two = train[[\"Pclass\",\"Age\",\"Sex\",\"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Control overfitting by setting \"max_depth\" to 10\n",
    "max_depth = 10\n",
    "\n",
    "# Control overfitting by setting \"min_samples_split\" to 5\n",
    "my_tree_two = tree.DecisionTreeClassifier(max_depth = 10, min_samples_split = 5, random_state = 1)\n",
    "\n",
    "# Create the my_tree_two model\n",
    "start = time()\n",
    "my_tree_two = my_tree_two.fit(features_two, target)\n",
    "\n",
    "#Print the score of the new decison tree\n",
    "print \"Our new Decision Tree prediction took {:.2f} seconds.\".format(time() - start)\n",
    "print \"Importance:\\n\", my_tree_two.feature_importances_\n",
    "print \"Score:\\n\", my_tree_two.score(features_two, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Even though the scope of this tutorial doesn't include actually submitting the updated solution to Kaggle, we would see however that despite a lower `.score`, this new model predicts better then the fist one. \n",
    "\n",
    "# Feature Engineering\n",
    "One of the most complicated aspects of Data Science is trying various machine learning algorithms, dealing with over and under-fitting and tweaking parameters to find the best possible fit to new and unseen data. Part of the process fo tweaking paramaters isfeature engineering. This is the process of creatively engineering our own features by combining the different existing variables.\n",
    "\n",
    "While feature engineering is a discipline in itself, too broad to be covered here in detail, we will have a look at a simple example by creating our very own new predictive attribute: `family_size`. A valid assumption is that larger families need more time to get together on a sinking ship, and hence have lower probability of surviving. Family size is determined by the variables `SibSp` and `Parch`, which indicate the number of family members a certain passenger is traveling with. So when doing feature engineering, we add a new variable `family_size`, which is the sum of `SibSp` and `Parch` plus one (for the observation itself), to the test and train set. To engineer this new feature, we do the following:\n",
    "\n",
    "1. Create a \"fresh\" `train` set called `train_two` that differs from `train` only by having an extra column with the engineered variable `family_size`.\n",
    "2. Add the new engineered variable `family_size` in addition to `Pclass`, `Sex`, `Age`, `Fare`, `SibSp` and `Parch` to a new set,  `features_three`.\n",
    "3. Create a new decision tree as `my_tree_three` and fit the decision tree with the new feature set,  `features_three`.\n",
    "4. Find the score of the new decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Decision Tree with `family_size` prediction took 0.00 seconds.\n",
      "Importance:\n",
      "[ 0.11181314  0.31088095  0.22213523  0.26463439  0.02335337  0.02185157\n",
      "  0.04533135]\n",
      "Score:\n",
      "0.979797979798\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Create train_two with the newly defined feature\n",
    "target = train[\"Survived\"].values\n",
    "train_two = train.copy()\n",
    "train_two[\"family_size\"] = train_two[\"SibSp\"] + train_two[\"Parch\"] + 1\n",
    "\n",
    "# Create a new feature set and add the new feature\n",
    "features_three = train_two[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"SibSp\", \"Parch\", \"family_size\"]].values\n",
    "\n",
    "# Define the tree classifier, then fit the model\n",
    "my_tree_three = tree.DecisionTreeClassifier()\n",
    "start = time()\n",
    "my_tree_three = my_tree_three.fit(features_three, target)\n",
    "\n",
    "# Print the score of this decision tree\n",
    "print \"Our Decision Tree with `family_size` prediction took {:.2f} seconds.\".format(time() - start)\n",
    "print \"Importance:\\n\", my_tree_three.feature_importances_\n",
    "print \"Score:\\n\", my_tree_three.score(features_three, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Notice__ that this time the newly created variable is included in the model. \n",
    "# Prediction using Random Forest\n",
    "A detailed study of Random Forests is outside the scope of this tutorial. However, since it's an often used machine learning technique, we introduce a general overview in Python. The Random Forest technique handles the overfitting problem we saw with decision trees. It grows multiple (very deep) classification trees using the training set. At the time of prediction, each tree is used to come up with a prediction and every outcome is counted as a vote. For example, if we have trained $3$ trees with $2$ saying a passenger in the test set will survive and $1$ says he will not, the passenger will be classified as a survivor. This approach of overtraining trees, but having the majority's vote count as the actual classification decision, avoids overfitting.\n",
    "\n",
    "Building a random forest in Python looks is very similar to building a decision tree, with three key differences.\n",
    "1. A different class is used.\n",
    "2. A new argument is necessary.\n",
    "3. The necessary library from `scikit-learn` must be imported.\n",
    "    - Use the `RandomForestClassifier()` class instead of the `DecisionTreeClassifier()` class.\n",
    "    - `n_estimators` needs to be set when using the `RandomForestClassifier()` class. This argument allows us to set the number of trees we wish to plant and average over.\n",
    "\n",
    "The following exampple shows us how to build a Random Forest Classifier by dowing the following:\n",
    "1. Build the random forest with `n_estimators` set to 100.\n",
    "2. Fit your random forest model with inputs features_forest and target.\n",
    "3. Compute the classifier predictions on the selected test set features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Random Forest prediction took 0.29 seconds.\n",
      "Score:\n",
      "0.939393939394\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Import the `RandomForestClassifier`\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creat a list of the features. \n",
    "target = train[\"Survived\"].values\n",
    "forest_features = train[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Building and fitting my_forest\n",
    "forest = RandomForestClassifier(max_depth = 10, min_samples_split=2, n_estimators = 100, random_state = 1)\n",
    "start = time()\n",
    "my_forest = forest.fit(forest_features, target)\n",
    "\n",
    "# Print the score of the fitted random forest\n",
    "print \"Our Random Forest prediction took {:.2f} seconds.\".format(time() - start)\n",
    "print \"Score:\\n\", my_forest.score(forest_features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model Comparison\n",
    "Recall that in the when using the Decision Tree models, we looked at the `.feature_importances_` attribute to see how each of the feature influenced the decision trees.  We can request the same attribute from the random forest as well and interpret the relevance of the included variables. Since the Random Forest aleviates the overfitting problem, it would be a good exercise to compare it to the Decision Tree model in some quick and easy way. For this, we can use the `.score()` method, which takes the features data and the target vector and computes mean accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree final score: 0.905723905724\n",
      "Decision Tree features sorted by their score:\n",
      "[(0.4162, 'Sex'), (0.1794, 'Fare'), (0.1791, 'Age'), (0.1413, 'Pclass'), (0.0504, 'SibSp'), (0.0192, 'Parch'), (0.0144, 'Embarked')]\n",
      "\n",
      "\n",
      "Random Forest final score: 0.939393939394\n",
      "Random Forest features sorted by their score:\n",
      "[(0.3199, 'Sex'), (0.246, 'Fare'), (0.2014, 'Age'), (0.1038, 'Pclass'), (0.0527, 'SibSp'), (0.0416, 'Parch'), (0.0345, 'Embarked')]\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Final score comparison\n",
    "names = [\"Pclass\",\"Age\",\"Sex\",\"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]\n",
    "print \"Decision Tree final score:\", my_tree_two.score(features_two, target)\n",
    "print \"Decision Tree features sorted by their score:\"\n",
    "print sorted(zip(map(lambda x: round(x, 4), my_tree_two.feature_importances_), names), \n",
    "             reverse=True)\n",
    "print \"\\n\"\n",
    "print \"Random Forest final score:\", my_forest.score(forest_features, target)\n",
    "print \"Random Forest features sorted by their score:\"\n",
    "print sorted(zip(map(lambda x: round(x, 4), my_forest.feature_importances_), names), \n",
    "             reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Conclusion\n",
    "Based on our findings from the various models that have been run, we can determine which feature was of most importance, and for which model.\n",
    "\n",
    "__The most important feature was \"Sex\", but it was more significant for \"my_tree_two\" Decision Tree.__\n",
    "\n",
    "---\n",
    "\n",
    "# Ensemble Methods\n",
    "In an prediction task, it is important try and test multiple algorithms to find the best possible fit. In most cases it is not necessary or possible to test every single algorithm type against the data, a fare assesment may suffice, but it is a good practice to at least try achieve a better fit. That is why we introduced a different Machine Learning algorithm to the `DecisionTreeClassifier()` we were currently using. What we did in essence is introduce an Ensemble Method. The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability or robustness over a single estimator. In our case, we used the `RandomForestClassifier()` which built several (very deep) classification trees that when used for prediction, each tree is used to come up with a prediction and every outcome is counted as a vote. This is an example of an __averaging methods__. Here, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced. This is evedent when comparing the score from both models above.\n",
    "\n",
    "One of the biggest issues with ensemble Methods is that they can be computationally exhaustive. The alternative is manually trying every possible variable combinations (since there is no easy way to know which parameters work best, other than trying out many different combinations) or parameter combinations (hyperparameters) to get the best model fit. Fortunately, the `scikit-learn` package includes the `GridSearchCV` and `RandomizedSearchCV` functions wich allows for the evaluation of each parameter setting independently, in parallel.\n",
    "\n",
    "The down-side to this is the fact that this is not scalable. The models above are executed on data in Pandas DataFrames. These DataFrames are memory resident, so unless the machine executing the models has sufficient memory or the data sets are small enough, the `GridSearchCV` and `RandomizedSearchCV` functions may not be helpful. Fortunately the team at Databrix has released the [`spark-sklearn`](http://spark-packages.org/package/databricks/spark-sklearn) package to allow us to execute these funcitons over a Spark cluster.\n",
    "\n",
    "The following example is based on the  [Auto-scaling scikit-learn with Spark](https://databricks.com/blog/2016/02/08/auto-scaling-scikit-learn-with-spark.html) example, but has been adapted to our Titanic example. The objective is to populate a \"grid\" of various model parameters and have `GridSearchCV` execute each set of parameters to determine the best score. The goal here is not necessarily to find a model with a better score but to illustrate the time it takes to search through the various model parameters to find the best one. The example has the following steps:\n",
    "\n",
    "1. Import the `GridSearchCV` library from `scikit-learn`.\n",
    "2. Create the function to report the top $3$ models, courtesy of [Databrix](http://go.databricks.com/hubfs/notebooks/Samples/Miscellaneous/blog_post_cv.html).\n",
    "3. Create the \"grid\" of parameters combnations to execute. Thjis is loosley based on the Databrix example.\n",
    "4. Fit the model with the \"grid\"of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 12.00 minutes for 960 candidate settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.835 (std: 0.019)\n",
      "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 40, 'criterion': 'gini', 'min_samples_split': 10, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.834 (std: 0.029)\n",
      "Parameters: {'bootstrap': True, 'min_samples_leaf': 2, 'n_estimators': 40, 'criterion': 'gini', 'min_samples_split': 10, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.833 (std: 0.016)\n",
      "Parameters: {'bootstrap': True, 'min_samples_leaf': 3, 'n_estimators': 10, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Import necessary packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Databrix utility function to report top 3 best scores\n",
    "def report(grid_scores, n_top = 3):\n",
    "    top_scores = sorted(grid_scores, key = itemgetter(1), reverse = True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "# Creat a list of the features. \n",
    "target = train[\"Survived\"].values\n",
    "forest_features = train[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Add grid parameter code based on Databricks\n",
    "param_grid = {\"max_depth\": [3, 10, None],\n",
    "              \"min_samples_split\": [1.0, 2, 3, 10], \n",
    "              \"min_samples_leaf\": [1, 2, 3, 10], \n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"n_estimators\": [10, 20, 40, 80, 160]}\n",
    "\n",
    "# Execute the grid search\n",
    "gs = GridSearchCV(RandomForestClassifier(), param_grid = param_grid)\n",
    "start = time()\n",
    "gs.fit(forest_features, target)\n",
    "print(\"GridSearchCV took {:.2f} minutes for {:d} candidate settings.\".format((time() - start) / 60, len(gs.grid_scores_)))\n",
    "report(gs.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "# Ensemble Methods at Scale\n",
    "As we can see, the `GridSearchCV` took $12$ minutes on a *m4.2xlarge* Instance and provided us with the top $3$ candidates and their settings to try. By applying these suggestions we can very effectivley find the best model as well as the optumum parameters to apply to achieve the best score. Next we aply the same proceedure to a Spark Cluster to parallelize the task across $5$ Spark Workers.\n",
    "\n",
    "## Provision EMR Cluster in Python\n",
    "\n",
    "```\n",
    "%%local\n",
    "import boto3\n",
    "connection = boto3.client(\n",
    "    'emr',\n",
    "    region_name='us-west-1',\n",
    "    aws_access_key_id='<Your AWS Access Key>',\n",
    "    aws_secret_access_key='<You AWS Secred Key>',\n",
    ")\n",
    "\n",
    "cluster_id = connection.run_job_flow(\n",
    "    Name='SparkaaS',\n",
    "    LogUri='s3://chkrd/emr-log',\n",
    "    ReleaseLabel='emr-5.4.0',\n",
    "    Instances={\n",
    "        'InstanceGroups': [\n",
    "            {\n",
    "                'Name': \"Master nodes\",\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'MASTER',\n",
    "                'InstanceType': 'm1.large',\n",
    "                'InstanceCount': 1,\n",
    "            },\n",
    "            {\n",
    "                'Name': \"Slave nodes\",\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'CORE',\n",
    "                'InstanceType': 'm1.large',\n",
    "                'InstanceCount': 5,\n",
    "            }\n",
    "        ],\n",
    "        'Ec2KeyName': '<Ec2 Keyname>',\n",
    "        'KeepJobFlowAliveWhenNoSteps': True,\n",
    "        'TerminationProtected': False,\n",
    "        'Ec2SubnetId': '<Your Subnet ID>',\n",
    "    },\n",
    "    Steps=[],\n",
    "    VisibleToAllUsers=True,\n",
    "    JobFlowRole='EMR_EC2_DefaultRole',\n",
    "    ServiceRole='EMR_DefaultRole',\n",
    "    Tags=[\n",
    "        {\n",
    "            'Key': 'tag_name_1',\n",
    "            'Value': 'tab_value_1',\n",
    "        },\n",
    "        {\n",
    "            'Key': 'tag_name_2',\n",
    "            'Value': 'tag_value_2',\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print (cluster_id['JobFlowId'])\n",
    "```\n",
    "## Provision EMR Cluster with AWS CLI\n",
    "```bash\n",
    "aws emr create-cluster --termination-protected --applications Name=Hadoop Name=Hive --ec2-attributes '{\"KeyName\":\"devenv-key\",\"InstanceProfile\":\"EMR_EC2_DefaultRole\",\"SubnetId\":\"subnet-39a22e5e\",\"EmrManagedSlaveSecurityGroup\":\"sg-86bb82fe\",\"EmrManagedMasterSecurityGroup\":\"sg-83bb82fb\"}' --release-label emr-5.4.0 --log-uri 's3n://chkrd/elasticmapreduce/' --steps '[{\"Args\":[\"nohup\",\"/home/hadoop/livy-server-0.3.0/bin/livy-server\",\">\",\"/dev/null\",\"2>/tmp/livy.log\",\"&\"],\"Type\":\"CUSTOM_JAR\",\"ActionOnFailure\":\"CONTINUE\",\"Jar\":\"command-runner.jar\",\"Properties\":\"\",\"Name\":\"Start Livy\"},{\"Args\":[\"aws\",\"s3\",\"cp\",\"s3://chkrd/artifacts/livy.conf\",\"/home/hadoop/livy-server-0.3.0/conf/\"],\"Type\":\"CUSTOM_JAR\",\"ActionOnFailure\":\"CONTINUE\",\"Jar\":\"command-runner.jar\",\"Properties\":\"\",\"Name\":\"Copy Livy Config\"},{\"Args\":[\"aws\",\"s3\",\"cp\",\"s3://chkrd/artifacts/livy-env.sh\",\"/home/hadoop/livy-server-0.3.0/conf/\"],\"Type\":\"CUSTOM_JAR\",\"ActionOnFailure\":\"CONTINUE\",\"Jar\":\"command-runner.jar\",\"Properties\":\"\",\"Name\":\"Copy Livy Environment\"}]' --instance-groups '[{\"InstanceCount\":1,\"InstanceGroupType\":\"MASTER\",\"InstanceType\":\"m3.xlarge\",\"Name\":\"Master - 1\"},{\"InstanceCount\":2,\"InstanceGroupType\":\"CORE\",\"InstanceType\":\"m3.xlarge\",\"Name\":\"Core - 2\"}]' --auto-scaling-role EMR_AutoScaling_DefaultRole --bootstrap-actions '[{\"Path\":\"s3://chkrd/artifacts/emr_base_config.sh\",\"Name\":\"EMR Base Configuration\"}]' --service-role EMR_DefaultRole --enable-debugging --name 'SparkaaS' --scale-down-behavior TERMINATE_AT_INSTANCE_HOUR --region us-west-2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load `sparkmagic` Jupyter Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Configure EMR Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added endpoint http://54.190.208.32:8998\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1489684169355_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-22-140.us-west-2.compute.internal:20888/proxy/application_1489684169355_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-24-106.us-west-2.compute.internal:8042/node/containerlogs/container_1489684169355_0001_01_000001/hadoop\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f210b815f50>"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.SQLContext object at 0x7f210b523f10>"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ensamble Methods using Spark-as-a-Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark GridSearchCV took 201.13 seconds for 960 candidate settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.835 (std: 0.019)\n",
      "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 40, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 10}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.834 (std: 0.025)\n",
      "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 20, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 10}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.833 (std: 0.019)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 2, 'n_estimators': 20, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 10}"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# Import necessary packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "\n",
    "# Reload the train and test datasets to create two DataFrames\n",
    "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "train = pd.read_csv(train_url)\n",
    "test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
    "test = pd.read_csv(test_url)\n",
    "\n",
    "# Preprocess the Data\n",
    "train[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\n",
    "train[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\n",
    "train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"S\"] = 0\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"C\"] = 1\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"Q\"] = 2\n",
    "\n",
    "# Databrix utility function to report top 3 best scores\n",
    "def report(grid_scores, n_top = 3):\n",
    "    top_scores = sorted(grid_scores, key = itemgetter(1), reverse = True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "# Creat a list of the features. \n",
    "target = train[\"Survived\"].values\n",
    "forest_features = train[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Add grid parameter code based on Databricks\n",
    "param_grid = {\"max_depth\": [3, 10, None],\n",
    "              \"min_samples_split\": [1.0, 2, 3, 10], \n",
    "              \"min_samples_leaf\": [1, 2, 3, 10], \n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"n_estimators\": [10, 20, 40, 80, 160]}\n",
    "\n",
    "# Import grid search for Spark\n",
    "from spark_sklearn import GridSearchCV as SparkSearch\n",
    "\n",
    "# Execute the grid search on Spark\n",
    "Spark_gs = SparkSearch(sc, RandomForestClassifier(), param_grid = param_grid)\n",
    "start = time()\n",
    "\n",
    "Spark_gs.fit(forest_features, target)\n",
    "print(\"Spark GridSearchCV took {:.2f} seconds for {:d} candidate settings.\".format(time() - start, len(Spark_gs.grid_scores_)))\n",
    "report(Spark_gs.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "By running the `GridSearchCV` on a cluster of $5$ Spark Nodes, we managed to execute the same ensemble task in $19$ seconds, thus demonstarting that distributing the various algorithms across multiple nodes certainly helps to narrow down the best fit without having to manually try multiple models and wasting unnecessary time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  },
  "widgets": {
   "state": {
    "00529b20c1a84330854d95077a9c50ac": {
     "views": []
    },
    "012e571a117745889e015b07f3cb04f4": {
     "views": []
    },
    "02ba446fa48a46689c6bcef8ab47335f": {
     "views": []
    },
    "0397d04a0b524e898135c0fe434dff42": {
     "views": []
    },
    "05677d9e98ae4d97bd865e183f6453bb": {
     "views": []
    },
    "0624d00202684f529cb23b3c613bdfb1": {
     "views": []
    },
    "06e6be2dcc3d406d8157a62cceb3e028": {
     "views": []
    },
    "0814337ae6394e6b8623f41bf6a8cd2f": {
     "views": []
    },
    "092d3843b883447d983541f2f6023211": {
     "views": []
    },
    "0beef7bf8d4341f2931f932759183c74": {
     "views": []
    },
    "0cf3442606164cafba96c1eef2166f41": {
     "views": []
    },
    "0dfe00e541824bfdaea6bc5aed630e87": {
     "views": []
    },
    "0e7d62afacf14967a65a7460972d0a46": {
     "views": []
    },
    "0f7c42fa1a4b4d008c7657445c6c4c9f": {
     "views": []
    },
    "12057679673341d9aba1d7595c773a47": {
     "views": []
    },
    "12827dcc33044c44a8e25e23aff27061": {
     "views": []
    },
    "1377835414a64be2afeb6de5a463e747": {
     "views": []
    },
    "1452474b828c4f01be76e8572c2ef53e": {
     "views": []
    },
    "179fea77a3204bc0a86656dae6b92db0": {
     "views": []
    },
    "18c411a04aed4f8e997529771a19e234": {
     "views": []
    },
    "1944a418a20e478ea3ee61680a700d37": {
     "views": []
    },
    "1a440520bdaa45d0a4ff3a3303993d59": {
     "views": []
    },
    "1a75c003ec08453b816bff80d6c7bd7d": {
     "views": []
    },
    "1a81703a7f6b4286a096c92c688e07b6": {
     "views": []
    },
    "1ace85a572ce4a87ba75a2f78858f7d8": {
     "views": []
    },
    "1bed4c56963646ed9ddd0a4972a78879": {
     "views": []
    },
    "1c00c62c81cc44329f4d2bed084910a1": {
     "views": []
    },
    "1c2bd2a37bbb43b9a62f750768b63f45": {
     "views": []
    },
    "1e578c2c8b324ac0a19fcd998b7243cb": {
     "views": []
    },
    "1eaed05d1c444a169cddc96da2ad6ede": {
     "views": []
    },
    "1f0e4c47bdba442c9154cad2017abf71": {
     "views": []
    },
    "1f78c78f199646d395b31ac8aa07985e": {
     "views": []
    },
    "200f7df840274a7fb0a70ebd4cae05df": {
     "views": []
    },
    "2329f523131549ecb11dc998cc057894": {
     "views": []
    },
    "23a765e103704deeb5868428a22dafdd": {
     "views": []
    },
    "2426b08d967e4f3795a243b113dfb721": {
     "views": []
    },
    "25dedb5efa9e4d068e80c6b12aa3ceac": {
     "views": []
    },
    "267bc52a7b7543c2a0978f1b76dc5c53": {
     "views": []
    },
    "2adbf2833c7b4142b11d4961f0fda562": {
     "views": []
    },
    "2ca100c734eb46ebae9061a3bf6db851": {
     "views": []
    },
    "2db0d6723c7d4aa79923195ba4150271": {
     "views": []
    },
    "3289cc54539244409a12faba437a3fea": {
     "views": []
    },
    "337f03652a77458cb443e40dfaeafc71": {
     "views": []
    },
    "33c1b8a64c9841c78fd4b2e9639d3df9": {
     "views": []
    },
    "3460d30820b04b05b36cfdf579fccb32": {
     "views": []
    },
    "34ee1991bd1f4908b7e94698bc9d1de2": {
     "views": []
    },
    "34ff3064870c414b853bf92eb5c362af": {
     "views": []
    },
    "35630199c40a4ac6ade7fadfe09cf040": {
     "views": []
    },
    "35e160f332df41899e843e7f9fbcb1de": {
     "views": []
    },
    "396d8678b8304056ac07dbf5aae10249": {
     "views": []
    },
    "3a3fb3a63dc04d6b867e02558f705456": {
     "views": []
    },
    "3e80520e3ca648a4abee6ba4aa3ac0d1": {
     "views": []
    },
    "3f660684161e46e094dab7783179dce6": {
     "views": []
    },
    "3f6e8b085e3642b08d6e5c7095f0e228": {
     "views": []
    },
    "418f532d9ff949b697593c8bb40d4847": {
     "views": []
    },
    "41b09d86834a41d68a9f1d672f662aaf": {
     "views": []
    },
    "4391ebc4e5e9455d86a9c6c3e80c42ec": {
     "views": []
    },
    "446517c65c244f989b23c3ed0ebdb365": {
     "views": []
    },
    "44ff6acfb5874f5e9a90bfd2f37fc3a8": {
     "views": []
    },
    "46824dc627a14c818b9da5adb1cb7df4": {
     "views": []
    },
    "47242779e39c4a42be81634fc0a894a4": {
     "views": []
    },
    "488320ff98174a4ea90b0fca66923aa6": {
     "views": []
    },
    "4916d15a5134497da4f6cd490ed52e14": {
     "views": []
    },
    "4bc2d2e1bc104793b4dafcb7e4df56da": {
     "views": []
    },
    "4cf311561d6341ccaf2aea3a9ba6577f": {
     "views": []
    },
    "4ec33801a2e74f45958487b93eea4615": {
     "views": []
    },
    "50163f46ab504c9cbaa23dbc64948049": {
     "views": []
    },
    "50bbcfb35f9c47f8b85cf0ec34091754": {
     "views": []
    },
    "51c58c9834ba43b8aff89db9bb1c2062": {
     "views": []
    },
    "53fb92fe1e84468f9370b8aa49a3545a": {
     "views": []
    },
    "5599a769da324b9a83853753813232e0": {
     "views": []
    },
    "55cdc59598804f3f968f994af419e7b1": {
     "views": []
    },
    "571c471d9629442e97889f510eb35f88": {
     "views": []
    },
    "5a2c9e86f5904052b4915cb8f2d1cd58": {
     "views": []
    },
    "5afaa3552faa4fc4994e3da39f2e2bd3": {
     "views": []
    },
    "5b03619546444ba8aa9070d9de22d11f": {
     "views": []
    },
    "5b5b37c7d6674706ab486e14b680bee4": {
     "views": []
    },
    "5cbda52808b144c2b57e9bfbca995bb5": {
     "views": []
    },
    "5cf89e9c29c647c3a8cd9f58a06a97b7": {
     "views": []
    },
    "5cfbdb14366e4d2da697902ef6b18838": {
     "views": []
    },
    "61dfaec5a27f4994803515beda650695": {
     "views": []
    },
    "64a72e6c9a16432fa465fb6e64d07b54": {
     "views": []
    },
    "64bafeb7c60549f5a1b6763fb87e8de2": {
     "views": []
    },
    "66ddc638369e4a7eb4fc3a5fafbd895a": {
     "views": []
    },
    "68a3682c620b448194c188520a5d1eb2": {
     "views": []
    },
    "6a3d419c4aca491ca44b4aaf980b0b16": {
     "views": []
    },
    "6ab2e0b0dcc9432e9fd20d2a7afc606e": {
     "views": []
    },
    "6b6fedf4f5fb4141b5b464430a1240d8": {
     "views": []
    },
    "7104d241b2ab43808961d460d5bc8269": {
     "views": []
    },
    "7216d1045b3f49f496cdf38a63e34888": {
     "views": []
    },
    "739be78c13994c028ab9447db3d9423a": {
     "views": []
    },
    "76d5c6ecb9304163a7f10b4829c2953e": {
     "views": []
    },
    "76f30df7a8354d3eb1d29e18bdc5b3f6": {
     "views": []
    },
    "79c0d997f82441c3bf55acf719b98376": {
     "views": []
    },
    "7a6ed4fa1d9d4fd992e1aa7360cd13d7": {
     "views": [
      {
       "cell_index": 46
      }
     ]
    },
    "7c4d4ece87a84805bd587b262745786f": {
     "views": []
    },
    "7e206ef033a24999a038f8ee6675a579": {
     "views": []
    },
    "8048b4d3bdaf49288b8b72a9179ad6a6": {
     "views": []
    },
    "8116caba2a6b40a08c7cf301c2028626": {
     "views": []
    },
    "836cc3b1d6c542dbab9a0fb97696e26d": {
     "views": []
    },
    "850203f46fb147b0ac96d1a5057ff07a": {
     "views": []
    },
    "852b87527c7f4ac48daa4c30a6516091": {
     "views": []
    },
    "8774239f99194ee99a91175851ec2318": {
     "views": []
    },
    "89cefd89be8942458ff98159a4b9914a": {
     "views": []
    },
    "89e19bd056cd4c309d6247820b7b8fcb": {
     "views": []
    },
    "8dd2ddeac6e044e0a634c55537ed6c6a": {
     "views": []
    },
    "8ec830cb255444dca51ae7d2422fa510": {
     "views": []
    },
    "913eb5aad2844f088f05d27c3ccb2465": {
     "views": []
    },
    "91e27403fdb84d8c9849e611244c9a0a": {
     "views": []
    },
    "92444eb61671420f93725f9b2aec6003": {
     "views": []
    },
    "92e8dd613d894d308865aa2e8823ed7b": {
     "views": []
    },
    "93ca182b4de241039da69489b39cfb37": {
     "views": []
    },
    "952108a5df4a4b3f993abe3dd4519b09": {
     "views": []
    },
    "9607ff3146c34e6aa98e0ca392674dab": {
     "views": []
    },
    "964b2e94dbbd4bb99adf0632e5cf37f4": {
     "views": []
    },
    "9763b1f3e0cf435c85afd30d64ccf656": {
     "views": []
    },
    "984c5ddd3c7c42768dcc899d291c1411": {
     "views": []
    },
    "9a620d7296544be58b22353dbf04e8ae": {
     "views": []
    },
    "9c02fd76ebaa401a9133d9c7b8e83446": {
     "views": []
    },
    "9cd82e2252904a1faa2f969b80dd3564": {
     "views": []
    },
    "9d05d0c7179a4977b52bcd6b7d1a791b": {
     "views": []
    },
    "a0ac43f265a1456a9663b89a814f06a4": {
     "views": []
    },
    "a1c9df848de34114b0518d98feffc639": {
     "views": []
    },
    "a2d998c1ec634c30ad7456f6584386b1": {
     "views": []
    },
    "a6a82d34f9ac47e0ab64b3bdffc492fb": {
     "views": []
    },
    "a6b97cf7ae794f49b3a24f1eac2f7a79": {
     "views": []
    },
    "a6d7871ada504fbb9c6caded9d8ae33d": {
     "views": []
    },
    "a732de4b4f1245febdc24c2b114c3456": {
     "views": []
    },
    "a7bb0cb9f5b04250a54701585cfac08c": {
     "views": []
    },
    "a950b90944624aa7969b2b5c8b27ca5a": {
     "views": []
    },
    "a9e42dbfe9d945e8988c0b0ee47553f6": {
     "views": []
    },
    "ac258c0a6c4b44cb83ee934bf8936f8b": {
     "views": []
    },
    "b195a443941740d6bce36a0aea94f643": {
     "views": []
    },
    "b1e01968cae84963b8f14424e6f6897c": {
     "views": []
    },
    "b219a2da9a86416c9efb70889acdb815": {
     "views": []
    },
    "b2e534950719494eba39b6076472e78b": {
     "views": []
    },
    "b3609e5ef7eb465a80051bc3d0b6c19d": {
     "views": []
    },
    "b381b5731546400bb775b31040c3ab11": {
     "views": []
    },
    "b3f41ac9e08f4c6495e9e337c7748b6c": {
     "views": []
    },
    "b51a7883988d4b2a8a00ecba951f4074": {
     "views": []
    },
    "b784155489a840f790424fb7125a2520": {
     "views": []
    },
    "b78dd89fdd6f4c179bbb5d2f209c8a95": {
     "views": []
    },
    "bb16224cc7054254bab7d587d4ff07e8": {
     "views": []
    },
    "bd23ce6ef5184078b872f5092f645a24": {
     "views": []
    },
    "bef4210239fb4a0d91f9f464b59b2db8": {
     "views": []
    },
    "bfc2243bcdc644248e3355cae0da9379": {
     "views": []
    },
    "c0b20e0a58fc4356bdcffcf2692e1beb": {
     "views": []
    },
    "c364e509493d4760b89bd27bceed58a0": {
     "views": []
    },
    "c4c907dc93194d3f87d639ded4372508": {
     "views": []
    },
    "c4d1fc4fa3024f3f8c55af78094bb15d": {
     "views": []
    },
    "c4e3009c69f24f10b94667289d419097": {
     "views": []
    },
    "c5695c0543e64c5799495b49a250ba3b": {
     "views": []
    },
    "c5bdb0ddbeff45c5bfafda9fd0dfc841": {
     "views": []
    },
    "c5e746a5a4e346f882c61cec3758400a": {
     "views": []
    },
    "c6033550dcd04549bae056d403580908": {
     "views": []
    },
    "c73911d71fe841f6a49e531d3e951dff": {
     "views": []
    },
    "c79f5731192a4e7a8b391521b3691551": {
     "views": []
    },
    "c7dc257489a84e229f1f4e9296eea6d8": {
     "views": []
    },
    "c8244850916948d5b4d037599814f48c": {
     "views": []
    },
    "cbcfa8d1f6e641e88acef7025441c732": {
     "views": []
    },
    "cbd1120b2b324075942680b769fbc260": {
     "views": []
    },
    "cbdcc49f55f64282904265704e2d8a31": {
     "views": []
    },
    "cdb6af57f4c5424c833fc54a393bff16": {
     "views": []
    },
    "ce284c6eb90245f5a1c742fbbfba194d": {
     "views": []
    },
    "cecd2f08396045df8f00ba3db29e63fd": {
     "views": []
    },
    "d181f63570364c86881c47962eaaa4df": {
     "views": []
    },
    "d1afdf59de754bff9a347ce46bb10be9": {
     "views": []
    },
    "d1e23674e91d4d8dba9f81bb9653fc59": {
     "views": []
    },
    "d29783ce11de4ae6a6d9a906aa558c64": {
     "views": []
    },
    "d2ee46e0ee7947efb8bfbeaa8e25a845": {
     "views": []
    },
    "d34234133f8247779dbb8f77368b836e": {
     "views": []
    },
    "d379628498b14964ade94eff91723140": {
     "views": []
    },
    "d693e02ac59f4dd8adc411bd7ec422b5": {
     "views": []
    },
    "d6a6b585d82541408a73df93b7bcf67b": {
     "views": []
    },
    "d80776bc3dd94b7e836fea9d47fff3f0": {
     "views": []
    },
    "d884b2aba981495f8b1d858b2717d83e": {
     "views": []
    },
    "d9893ab285e4454585b0faf15478de51": {
     "views": []
    },
    "dacf18d24e6c4ac3bca1f2e0f35eca91": {
     "views": []
    },
    "dbd6a8c7e3cb41a4b37154f8b9c0dafc": {
     "views": []
    },
    "dde1c305a73d4ca589e53e7aa221bfee": {
     "views": []
    },
    "dfa8b27042004dda8eb08d8b282cc2f2": {
     "views": []
    },
    "e05a828768d6476cb93385f7f713b07e": {
     "views": []
    },
    "e09a3a600c0142bfb86d29dc44b5c96f": {
     "views": []
    },
    "e0bf6955c9e443cc9ca923ab72796abb": {
     "views": []
    },
    "e205e38418c34d25bfed6324aef8a84f": {
     "views": []
    },
    "e6839d6ec52b4c4085dec1edbff000ff": {
     "views": []
    },
    "e701846309da4745975d248619808fa3": {
     "views": []
    },
    "e87fac95538a478b8fde62e4e68f809f": {
     "views": []
    },
    "e8c341a35a90426a9daa758831028fcf": {
     "views": []
    },
    "e9cc2a6b84424f6a89c6c83059f324e6": {
     "views": []
    },
    "eb197296bd934fe3ba548131291f0c61": {
     "views": []
    },
    "eb7e261267244afca4daa06d8ba014c0": {
     "views": []
    },
    "ec20b934668346b19c44ac75f796ef95": {
     "views": []
    },
    "edf52d7d4558414bbd85159e258b6e26": {
     "views": []
    },
    "f0dd2264f2fe4b169703d3b9e45a0810": {
     "views": []
    },
    "f48094934ff042229b31c7a9949d353d": {
     "views": []
    },
    "f55c7cc066d04fc295b749276d73be87": {
     "views": []
    },
    "f63c211e33f440f1a30828bae3006b0c": {
     "views": []
    },
    "f69baf2969ca4c7a8fdcd5e0ed9a4fcb": {
     "views": []
    },
    "f9cd8d570ff5479eb2f9af6c233901fa": {
     "views": []
    },
    "fa60f3c476bb4ca8a859df750debe25b": {
     "views": []
    },
    "fa9250e6d0c7497181b4db5a21cb37ee": {
     "views": []
    },
    "fcf25d2811094eac8adb8032bc7615b7": {
     "views": []
    },
    "fd6ebd19269f47378f61efaa9a0fb18d": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
